{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "4GvLiCvCH37c",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5473c9d8-58b8-4057-9977-f976718d22d2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523813151190,
          "user_tz": 240,
          "elapsed": 1025,
          "user": {
            "displayName": "Tiffany Wang",
            "photoUrl": "//lh3.googleusercontent.com/-DxivnaZLcsI/AAAAAAAAAAI/AAAAAAAAH_I/QDov37LUsKk/s50-c-k-no/photo.jpg",
            "userId": "108532619344838745606"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ccDoliY5II8h",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "code"
      },
      "cell_type": "code",
      "source": [
        "#@title Layers\n",
        "SEED = 123456\n",
        "rng = np.random.RandomState(SEED)\n",
        "\n",
        "\n",
        "def __createWeights(shape, seed=None, name='weight'):\n",
        "    w_init = tf.contrib.layers.variance_scaling_initializer(seed=seed)\n",
        "    return tf.get_variable(name + '_w', shape=shape, initializer=w_init)\n",
        "\n",
        "\n",
        "def __createBiases(size, name='bias'):\n",
        "    return tf.get_variable(name + '_b', shape=[size], initializer=tf.constant_initializer(0.0))\n",
        "\n",
        "\n",
        "def LeakyReLU(x, alpha=0.1):\n",
        "    x = tf.nn.leaky_relu(x, alpha=alpha)\n",
        "    return x\n",
        "\n",
        "\n",
        "def MaxPooling(x, ksize=2, stride_length=2, padding='SAME', data_format='NHWC'):\n",
        "    x = tf.nn.max_pool(x, (1, ksize, ksize, 1), (1, stride_length, stride_length, 1), padding, data_format)\n",
        "    return x\n",
        "\n",
        "\n",
        "def GlobalAveragePooling(x):\n",
        "    x = tf.reduce_mean(x, [1, 2])\n",
        "    return x\n",
        "\n",
        "\n",
        "def Dense(x, input_dim, output_dim, seed=None, name='dense'):\n",
        "    W = __createWeights([input_dim, output_dim], seed, name) \n",
        "    b = __createBiases(output_dim, name) \n",
        "    x = tf.nn.xw_plus_b(x, W, b)\n",
        "    return x\n",
        "\n",
        "\n",
        "def Conv2D(x, filter_size, n_channels, n_filters, stride_length=1, padding='SAME', data_format='NHWC', name='conv'):\n",
        "    shape = [filter_size, filter_size, n_channels, n_filters]\n",
        "    W = __createWeights(shape, name=name)\n",
        "    b = __createBiases(n_filters, name=name)\n",
        "    x = tf.nn.conv2d(x, filter=W, strides=(1, stride_length, stride_length, 1), padding=padding, data_format=data_format)\n",
        "    x += b\n",
        "    return x\n",
        "\n",
        "\n",
        "def Dropout(x, probability=0.5):\n",
        "    x = tf.nn.dropout(x, keep_prob=probability, seed=rng.randint(SEED))\n",
        "    return x\n",
        "\n",
        "\n",
        "def GaussianNoise(x, sigma=0.15):\n",
        "    noise = tf.random_normal(shape=tf.shape(x), stddev=sigma)\n",
        "    x += noise\n",
        "    return x\n",
        "\n",
        "\n",
        "def SoftMax(x):\n",
        "    x = tf.nn.softmax(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Loss functions. Arg 1: Approximation, Arg 2: Labels\n",
        "'''\n",
        "def CrossEntropyWithLogits(logits, labels):\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Formula: sum(p_i * log(p_i) - p_i * log(q_i))\n",
        "def KLDivergenceWithLogits(q, p):\n",
        "    p_soft = SoftMax(p)\n",
        "    # plogp = tf.reduce_mean(tf.reduce_sum(p_soft * tf.nn.log_softmax(p), 1))\n",
        "    # plogq = tf.reduce_mean(tf.reduce_sum(p_soft * tf.nn.log_softmax(q), 1))\n",
        "    distance = tf.reduce_sum(p_soft * tf.nn.log_softmax(p) - p_soft * tf.nn.log_softmax(q))\n",
        "    # distance = plogp - plogq\n",
        "    return distance\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jl1tdmmMIUlU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "code"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "The model before AdD is applied\n",
        "'''\n",
        "def upperBlock(x, conv_size=[128, 256, 512, 256,128]):\n",
        "    x =  GaussianNoise(x)\n",
        "    x =  Conv2D(x, filter_size=3, n_channels=1, n_filters=conv_size[0], padding='SAME', name='1a')\n",
        "    x =  LeakyReLU(x)\n",
        "    x =  Conv2D(x, filter_size=3, n_channels=conv_size[0], n_filters=conv_size[0], name='1b')\n",
        "    x =  LeakyReLU(x)\n",
        "    x =  Conv2D(x, filter_size=3, n_channels=conv_size[0], n_filters=conv_size[0], name='1c')\n",
        "    x =  MaxPooling(x, ksize=2, stride_length=2)\n",
        "    x =  Dropout(x, probability=0.5)\n",
        "    \n",
        "    x =  Conv2D(x, filter_size=3, n_channels=conv_size[0], n_filters=conv_size[1], name='2a')\n",
        "    x =  LeakyReLU(x)\n",
        "    x =  Conv2D(x, filter_size=3, n_channels=conv_size[1], n_filters=conv_size[1], name='2b')\n",
        "    x =  LeakyReLU(x)\n",
        "    x =  Conv2D(x, filter_size=3, n_channels=conv_size[1], n_filters=conv_size[1], name='2c')\n",
        "    x =  LeakyReLU(x)\n",
        "    x =  MaxPooling(x, ksize=2, stride_length=2)\n",
        "\n",
        "    x =  Conv2D(x, filter_size=3, n_channels=conv_size[1], n_filters=conv_size[2], padding='VALID', name='3a')\n",
        "    x =  LeakyReLU(x)\n",
        "    x =  Conv2D(x, filter_size=1, n_channels=conv_size[2], n_filters=conv_size[3], name='3b')\n",
        "    x =  LeakyReLU(x)\n",
        "    x =  Conv2D(x, filter_size=1, n_channels=conv_size[3], n_filters=conv_size[4], name='3c')\n",
        "    x =  LeakyReLU(x)\n",
        "    x =  GlobalAveragePooling(x) \n",
        "    \n",
        "    # x =  Dropout(x, probability=0.5)\n",
        "    # x =  Dense(x, conv_size[4], 10)\n",
        "    # x =  SoftMax(x)\n",
        "\n",
        "    return x\n",
        "  \n",
        "def upperBlock_std(x, conv_size=[128, 256, 512, 256,128]):\n",
        "    x =  GaussianNoise(x)\n",
        "    x =  Conv2D(x, filter_size=3, n_channels=1, n_filters=conv_size[0], padding='SAME', name='1a_std')\n",
        "    x =  LeakyReLU(x)\n",
        "    x =  Conv2D(x, filter_size=3, n_channels=conv_size[0], n_filters=conv_size[0], name='1b_std')\n",
        "    x =  LeakyReLU(x)\n",
        "    x =  Conv2D(x, filter_size=3, n_channels=conv_size[0], n_filters=conv_size[0], name='1c_std')\n",
        "    x =  MaxPooling(x, ksize=2, stride_length=2)\n",
        "    x =  Dropout(x, probability=0.5)\n",
        "    \n",
        "    x =  Conv2D(x, filter_size=3, n_channels=conv_size[0], n_filters=conv_size[1], name='2a_std')\n",
        "    x =  LeakyReLU(x)\n",
        "    x =  Conv2D(x, filter_size=3, n_channels=conv_size[1], n_filters=conv_size[1], name='2b_std')\n",
        "    x =  LeakyReLU(x)\n",
        "    x =  Conv2D(x, filter_size=3, n_channels=conv_size[1], n_filters=conv_size[1], name='2c_std')\n",
        "    x =  LeakyReLU(x)\n",
        "    x =  MaxPooling(x, ksize=2, stride_length=2)\n",
        "\n",
        "    x =  Conv2D(x, filter_size=3, n_channels=conv_size[1], n_filters=conv_size[2], padding='VALID', name='3a_std')\n",
        "    x =  LeakyReLU(x)\n",
        "    x =  Conv2D(x, filter_size=1, n_channels=conv_size[2], n_filters=conv_size[3], name='3b_std')\n",
        "    x =  LeakyReLU(x)\n",
        "    x =  Conv2D(x, filter_size=1, n_channels=conv_size[3], n_filters=conv_size[4], name='3c_std')\n",
        "    x =  LeakyReLU(x)\n",
        "    x =  GlobalAveragePooling(x) \n",
        "    \n",
        "    # x =  Dropout(x, probability=0.5)\n",
        "    # x =  Dense(x, conv_size[4], 10)\n",
        "    # x =  SoftMax(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "'''\n",
        "The model after AdD is applied\n",
        "'''\n",
        "def lowerBlock(x, n_in=128, n_out=10, name='fc'):\n",
        "    x =  Dense(x, n_in, n_out, name=name)\n",
        "    return x;\n",
        "\n",
        "def lowerBlock_std(x, n_in=128, n_out=10, name='fc_std'):\n",
        "    x =  Dense(x, n_in, n_out, name=name)\n",
        "    return x;\n",
        "\n",
        "\n",
        "'''\n",
        "Apply adv dropout\n",
        "'''\n",
        "def advDropout(x, mask, Jacobian, sigma=0.05, dim=128):\n",
        "    # y: output \n",
        "    # mask: current sampled dropout mask \n",
        "    # sigma: hyper-parameter for boundary \n",
        "    # Jabocian: Jacobian vector (gradient of divergence (or loss function))\n",
        "    # dim: layer dimension \n",
        "\n",
        "    Jacobian = tf.reshape(Jacobian, [-1, dim])\n",
        "\n",
        "    # mask = 0 --> -1 \n",
        "    mask = 2 * mask - tf.ones_like(mask)\n",
        "\n",
        "    adv_mask = mask \n",
        "\n",
        "    # extract the voxels for which the update conditions hold \n",
        "    # mask = 0 and J > 0 \n",
        "    # or\n",
        "    # mask = 1 and J < 1 \n",
        "    abs_jac = tf.abs(Jacobian)\n",
        "    temp = tf.cast(tf.greater(abs_jac, 0), tf.float32)\n",
        "    temp = 2 * temp - 1 \n",
        "    # interested in the cases when temp * mask = -1\n",
        "    ext = tf.cast(tf.less(mask, temp), tf.float32)\n",
        "\n",
        "    # keep the voxels that you want to update \n",
        "    candidates = abs_jac * ext \n",
        "    thres = tf.nn.top_k(candidates, int(dim * sigma * sigma)  + 1)[0][:,-1]\n",
        "\n",
        "    targets = tf.cast(tf.greater(candidates, tf.expand_dims(thres, -1)), tf.float32)\n",
        "\n",
        "    # get new mask \n",
        "    adv_mask = (mask - targets * 2 * mask + tf.ones_like(mask)) / 2.0\n",
        "\n",
        "    output = adv_mask * x\n",
        "\n",
        "    return output, adv_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DjPBhjapIYFE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "code"
      },
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "'''\n",
        "Returns a model without adversarial dropout\n",
        "'''\n",
        "def modelWithRandD_std(x):\n",
        "    x = upperBlock_std(x)\n",
        "    x = lowerBlock_std(x)\n",
        "    return x\n",
        "\n",
        "  \n",
        "def modelWithRandD(x):\n",
        "    x = upperBlock(x)\n",
        "    x = lowerBlock(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "'''\n",
        "Returns a model with adversarial dropout\n",
        "'''\n",
        "def modelWithAdD(x, y, fn_loss=KLDivergenceWithLogits):\n",
        "    x = upperBlock(x)\n",
        "    y_no_adD = lowerBlock(x)\n",
        "    loss_no_adD = fn_loss(y_no_adD, y)\n",
        "\n",
        "    # Derivative of loss fn wrt x\n",
        "    DLoss = tf.gradients(loss_no_adD, [x])\n",
        "    DLoss = tf.squeeze(tf.stop_gradient(DLoss)) # Stops backpropagation\n",
        "\n",
        "    Jacobian_approx = DLoss * x\n",
        "    mask = tf.ones_like(x)\n",
        "\n",
        "    x, _ = advDropout(x, mask, Jacobian_approx)\n",
        "    x = lowerBlock(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "def CreateBaseModel(x, y, learning_rate=0.001, optimizer=tf.train.AdamOptimizer, momentum=0.999):\n",
        "    logit_rand = modelWithRandD_std(x)\n",
        "    loss = CrossEntropyWithLogits(logit_rand, y)\n",
        "\n",
        "    opt = optimizer(learning_rate=learning_rate, beta1=momentum)\n",
        "    gradients = opt.compute_gradients(loss, tf.trainable_variables())\n",
        "    train_op = opt.apply_gradients(gradients)\n",
        "\n",
        "    return train_op, loss, logit_rand\n",
        "\n",
        "\n",
        "'''\n",
        "Create the AdD model for training\n",
        "'''\n",
        "def CreateAdDModel(x, y, learning_rate=0.001, optimizer=tf.train.AdamOptimizer, momentum=0.999, lmb=0.01):\n",
        "    logit_rand = modelWithRandD(x)\n",
        "    logit_rand_loss = CrossEntropyWithLogits(logit_rand, y)\n",
        "\n",
        "    with tf.variable_scope(tf.get_variable_scope(), reuse=True) as scope:\n",
        "        # With adversarial dropout\n",
        "        logit_adD = modelWithAdD(x, y)\n",
        "        logit_adD_loss = CrossEntropyWithLogits(logit_adD, y)\n",
        "\n",
        "        # Total loss\n",
        "        loss = logit_rand_loss + lmb * logit_adD_loss\n",
        "\n",
        "\n",
        "    opt = optimizer(learning_rate=learning_rate, beta1=momentum)\n",
        "    gradients = opt.compute_gradients(loss, tf.trainable_variables())\n",
        "    train_op = opt.apply_gradients(gradients)        \n",
        "\n",
        "    return train_op, loss, logit_rand\n",
        "\n",
        "\n",
        "def createTestModel(x, y):\n",
        "    with tf.variable_scope(tf.get_variable_scope(), reuse=True) as scope:\n",
        "        logit_rand = modelWithRandD(x)\n",
        "        logit_rand_loss = CrossEntropyWithLogits(logit_rand, y)\n",
        "\n",
        "        return logit_rand, logit_rand_loss\n",
        "\n",
        "\n",
        "def Accuracy(logits, labels):\n",
        "    y_pred = tf.argmax(logits, 1)\n",
        "    y_true = tf.argmax(labels, 1)\n",
        "    equality = tf.equal(y_pred, y_true)\n",
        "    accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def visualize(acc, loss, param):\n",
        "    x = np.arange(0, len(acc))\n",
        "    name = \"Baseline\" if param['BASELINE'] else \"Adversarial\"\n",
        "    plt.title(\"Acc trend: \" + name)\n",
        "    plt.plot(x, acc)\n",
        "    filename = str(\"Accuracy_\" + re.sub(\"{|}|:|'|,| \", \"\", str(param)) + \".png\")\n",
        "    print(filename)\n",
        "    plt.savefig(filename)\n",
        "#     files.download(filename)\n",
        "    \n",
        "    plt.close()\n",
        "\n",
        "    plt.title(\"Loss trend: \" + name)\n",
        "    plt.plot(x, loss)\n",
        "    filename = str(\"Loss_\" + re.sub(\"{|}|:|'|,| \", \"\", str(param)) + \".png\")\n",
        "    print(filename)\n",
        "    plt.savefig(filename)\n",
        "#     files.download(filename)\n",
        "    \n",
        "    plt.close()\n",
        "    \n",
        "def visualize_both(acc, loss, acc_base, loss_base, param):\n",
        "    x = np.arange(0, len(acc))\n",
        "    name = param['TYPE']\n",
        "    plt.title(\"Acc trend: \" + name)\n",
        "    plt.plot(x, acc, 'r')\n",
        "    plt.plot(x, acc_base, 'b')\n",
        "    filename = str(\"Accuracy_\" + re.sub(\"{|}|:|'|,| \", \"\", str(param)) + \".png\")\n",
        "    print(filename)\n",
        "    plt.savefig(filename)\n",
        "#     files.download(filename)\n",
        "    \n",
        "    plt.close()\n",
        "\n",
        "    plt.title(\"Loss trend: \" + name)\n",
        "    plt.plot(x, loss, 'r')\n",
        "    plt.plot(x, loss_base, 'b')\n",
        "    filename = str(\"Loss_\" + re.sub(\"{|}|:|'|,| \", \"\", str(param)) + \".png\")\n",
        "    print(filename)\n",
        "    plt.savefig(filename)\n",
        "#     files.download(filename)\n",
        "    \n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "def doTraining(x_train, y_train, x_test, y_test, param):\n",
        "    # Training setup\n",
        "\n",
        "    batch_size = param['BATCH_SIZE']\n",
        "    epochs = param['EPOCHS']\n",
        "\n",
        "    STEPS = len(x_train) // batch_size if param['STEPS'] is None else param['STEPS']\n",
        "    TEST_STEPS = len(x_test) // batch_size if param['STEPS'] is None else param['STEPS']\n",
        "\n",
        "    # Graph\n",
        "    x_train_ph = tf.placeholder(tf.float32)\n",
        "    x_test_ph = tf.placeholder(tf.float32)\n",
        "    y_train_ph = tf.placeholder(tf.float32)\n",
        "    y_test_ph = tf.placeholder(tf.float32)\n",
        "\n",
        "    train_op, loss, logit_rand = CreateAdDModel(x_train_ph, y_train_ph)\n",
        "    train_op_base, loss_base, logit_rand_base = CreateBaseModel(x_train_ph, y_train_ph) \n",
        "    logit_test, test_loss = createTestModel(x_test_ph, y_test_ph)\n",
        "    logit_test_base, test_loss_base = createTestModel(x_test_ph, y_test_ph)\n",
        "\n",
        "\n",
        "    # Accuracy Train\n",
        "    accuracy_train = Accuracy(logit_rand, y_train_ph)\n",
        "    accuracy_train_base = Accuracy(logit_rand_base, y_train_ph)\n",
        "    \n",
        "\n",
        "    # Accuracy Test\n",
        "    accuracy_test = Accuracy(logit_test, y_test_ph)\n",
        "    accuracy_test_base = Accuracy(logit_test_base, y_test_ph)\n",
        "    \n",
        "\n",
        "    acc_train_trend = []\n",
        "    loss_train_trend = []\n",
        "    acc_test_trend = []\n",
        "    loss_test_trend = []\n",
        "\n",
        "    \n",
        "    acc_train_trend_base = []\n",
        "    loss_train_trend_base = []\n",
        "    acc_test_trend_base = []\n",
        "    loss_test_trend_base = []\n",
        "\n",
        "    \n",
        "    \n",
        "    # Training\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Train model\n",
        "            acc_train, loss_train = 0, 0\n",
        "            for i in range(STEPS):\n",
        "                _, loss_, acc = sess.run([train_op, loss, accuracy_train],\n",
        "                                        feed_dict={x_train_ph: x_train[batch_size * i: batch_size * (i + 1)],\n",
        "                                                   y_train_ph: y_train[batch_size * i: batch_size * (i + 1)]})\n",
        "                acc_train += acc\n",
        "                loss_train += loss_\n",
        "                \n",
        "            acc_train_trend.append(acc_train)\n",
        "            loss_train_trend.append(loss_train)\n",
        "\n",
        "            # Test model\n",
        "            acc_test, loss_test = 0, 0\n",
        "            for i in range(TEST_STEPS):\n",
        "                acc_t, loss_t = sess.run([accuracy_test, test_loss],\n",
        "                                         feed_dict={x_test_ph: x_test[batch_size * i: batch_size * (i + 1)],\n",
        "                                                    y_test_ph: y_test[batch_size * i: batch_size * (i + 1)]})\n",
        "                \n",
        "                acc_test += acc_t\n",
        "                loss_test += loss_t\n",
        "\n",
        "            acc_test_trend.append(acc_test)\n",
        "            loss_test_trend.append(loss_test)\n",
        "            \n",
        "            print('ADV : Epoch: {} || Train Loss: {},\\t Train Acc: {} || Test Loss: {},\\t Test Accuracy: {}'.format(epoch,\n",
        "                                                                                                          round(loss_train, 10) / STEPS,\n",
        "                                                                                                          round(acc_train, 10) / STEPS,\n",
        "                                                                                                          round(loss_test, 10) / TEST_STEPS,\n",
        "                                                                                                          round(acc_test, 10) / TEST_STEPS))\n",
        "        for epoch in range(epochs):\n",
        "            # Train model\n",
        "            acc_train_base, loss_train_base = 0, 0\n",
        "            for i in range(STEPS):\n",
        "                \n",
        "                _, loss_b, acc_b = sess.run([train_op_base, loss_base, accuracy_train_base],\n",
        "                                        feed_dict={x_train_ph: x_train[batch_size * i: batch_size * (i + 1)],\n",
        "                                                   y_train_ph: y_train[batch_size * i: batch_size * (i + 1)]})\n",
        "                \n",
        "                acc_train_base += acc_b\n",
        "                loss_train_base += loss_b\n",
        "\n",
        "            acc_train_trend_base.append(acc_train_base)\n",
        "            loss_train_trend_base.append(loss_train_base)\n",
        "\n",
        "            # Test model\n",
        "            acc_test_base, loss_test_base = 0, 0\n",
        "            for i in range(TEST_STEPS):\n",
        "                \n",
        "                acc_t_b, loss_t_b = sess.run([accuracy_test_base, test_loss_base], \n",
        "                                             feed_dict={x_test_ph: x_test[batch_size * i: batch_size * (i + 1)],\n",
        "                                                    y_test_ph: y_test[batch_size * i: batch_size * (i + 1)]})\n",
        "                \n",
        "                acc_test_base += acc_t_b\n",
        "                loss_test_base += loss_t_b\n",
        "\n",
        "            acc_test_trend_base.append(acc_test_base)\n",
        "            loss_test_trend_base.append(loss_test_base)\n",
        "      \n",
        "      \n",
        "      \n",
        "            print('BASELINE: Epoch: {} || Train Loss: {},\\t Train Acc: {} || Test Loss: {},\\t Test Accuracy: {}'.format(epoch,\n",
        "                                                                                                          round(loss_train_base, 10) / STEPS,\n",
        "                                                                                                          round(acc_train_base, 10) / STEPS,\n",
        "                                                                                                          round(loss_test_base, 10) / TEST_STEPS,\n",
        "                                                                                                          round(acc_test_base, 10) / TEST_STEPS))\n",
        "      \n",
        "    acc_train_trend = np.asarray(acc_train_trend) / STEPS\n",
        "    loss_train_trend = np.asarray(loss_train_trend) / STEPS\n",
        "    acc_test_trend = np.asarray(acc_test_trend) / TEST_STEPS\n",
        "    loss_test_trend = np.asarray(loss_test_trend) / TEST_STEPS\n",
        "    \n",
        "    acc_train_trend_base = np.asarray(acc_train_trend_base) / STEPS\n",
        "    loss_train_trend_base = np.asarray(loss_train_trend_base) / STEPS\n",
        "    acc_test_trend_base = np.asarray(acc_test_trend_base) / TEST_STEPS\n",
        "    loss_test_trend_base = np.asarray(loss_test_trend_base) / TEST_STEPS\n",
        "    \n",
        "    \n",
        "    # Train\n",
        "    param['TYPE'] = 'train'\n",
        "    param['STEPS'] = STEPS\n",
        "    visualize_both(acc_train_trend, loss_train_trend, acc_train_trend_base, loss_train_trend_base, param)\n",
        "\n",
        "    # Test\n",
        "    param['TYPE'] = 'test'\n",
        "    param['STEPS'] = TEST_STEPS\n",
        "    visualize_both(acc_test_trend, loss_test_trend, acc_test_trend_base, loss_test_trend_base, param)\n",
        "\n",
        "    return acc_train_trend, loss_train_trend, acc_test_trend, loss_test_trend, acc_train_trend_base, loss_train_trend_base, acc_test_trend_base, loss_test_trend_base\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xfN63Y6HIftx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1285
        },
        "outputId": "04e9480e-eea0-4c8d-d25c-c14e0192bb8e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523818242132,
          "user_tz": 240,
          "elapsed": 5087200,
          "user": {
            "displayName": "Tiffany Wang",
            "photoUrl": "//lh3.googleusercontent.com/-DxivnaZLcsI/AAAAAAAAAAI/AAAAAAAAH_I/QDov37LUsKk/s50-c-k-no/photo.jpg",
            "userId": "108532619344838745606"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "  with tf.Session() as sess:\n",
        "      x_train = sess.run(tf.image.rgb_to_grayscale(x_train))\n",
        "      x_test = sess.run(tf.image.rgb_to_grayscale(x_test))\n",
        "\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  param = {\n",
        "      'BATCH_SIZE': 128,\n",
        "      'EPOCHS': 20,\n",
        "      'STEPS': None\n",
        "  }\n",
        "  doTraining(x_train, y_train, x_test, y_test, param)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ADV : Epoch: 0 || Train Loss: 113.71768614451078,\t Train Acc: 0.11143830128205129 || Test Loss: 16.514662498083332,\t Test Accuracy: 0.1084735576923077\n",
            "ADV : Epoch: 1 || Train Loss: 11.538705243819743,\t Train Acc: 0.12061298076923077 || Test Loss: 9.355403117644872,\t Test Accuracy: 0.15354567307692307\n",
            "ADV : Epoch: 2 || Train Loss: 9.07605478335641,\t Train Acc: 0.13565705128205127 || Test Loss: 5.300556892003846,\t Test Accuracy: 0.15324519230769232\n",
            "ADV : Epoch: 3 || Train Loss: 6.550384693879231,\t Train Acc: 0.15494791666666666 || Test Loss: 6.4412874075076925,\t Test Accuracy: 0.14973958333333334\n",
            "ADV : Epoch: 4 || Train Loss: 6.501123268787692,\t Train Acc: 0.15392628205128206 || Test Loss: 5.455359098238461,\t Test Accuracy: 0.14943910256410256\n",
            "ADV : Epoch: 5 || Train Loss: 4.738488745077949,\t Train Acc: 0.15891426282051282 || Test Loss: 3.0965797014730767,\t Test Accuracy: 0.16836939102564102\n",
            "ADV : Epoch: 6 || Train Loss: 3.5721076983671796,\t Train Acc: 0.17866586538461537 || Test Loss: 3.4915011234769233,\t Test Accuracy: 0.17958733974358973\n",
            "ADV : Epoch: 7 || Train Loss: 2.9253202120464104,\t Train Acc: 0.19022435897435896 || Test Loss: 2.7128132337179487,\t Test Accuracy: 0.18689903846153846\n",
            "ADV : Epoch: 8 || Train Loss: 2.7679979550533336,\t Train Acc: 0.2048477564102564 || Test Loss: 2.6562166183423077,\t Test Accuracy: 0.19310897435897437\n",
            "ADV : Epoch: 9 || Train Loss: 2.522805801110513,\t Train Acc: 0.21784855769230768 || Test Loss: 2.371487975120513,\t Test Accuracy: 0.2146434294871795\n",
            "ADV : Epoch: 10 || Train Loss: 2.388651926089487,\t Train Acc: 0.23317307692307693 || Test Loss: 2.106971234847436,\t Test Accuracy: 0.2783453525641026\n",
            "ADV : Epoch: 11 || Train Loss: 2.388331617453077,\t Train Acc: 0.2416065705128205 || Test Loss: 2.0720320298115387,\t Test Accuracy: 0.26943108974358976\n",
            "ADV : Epoch: 12 || Train Loss: 2.3257096495384615,\t Train Acc: 0.25340544871794873 || Test Loss: 2.050083675446154,\t Test Accuracy: 0.2913661858974359\n",
            "ADV : Epoch: 13 || Train Loss: 2.2572905983679488,\t Train Acc: 0.2635616987179487 || Test Loss: 2.296609707367949,\t Test Accuracy: 0.23347355769230768\n",
            "ADV : Epoch: 14 || Train Loss: 2.476794439095641,\t Train Acc: 0.2440705128205128 || Test Loss: 2.3835518910333335,\t Test Accuracy: 0.27013221153846156\n",
            "ADV : Epoch: 15 || Train Loss: 2.291272265788718,\t Train Acc: 0.26137820512820514 || Test Loss: 2.192533384530769,\t Test Accuracy: 0.2428886217948718\n",
            "ADV : Epoch: 16 || Train Loss: 2.177425660536923,\t Train Acc: 0.2744190705128205 || Test Loss: 2.1848426262538463,\t Test Accuracy: 0.30859375\n",
            "ADV : Epoch: 17 || Train Loss: 2.1495441852471795,\t Train Acc: 0.26981169871794874 || Test Loss: 2.11202041919359,\t Test Accuracy: 0.2761418269230769\n",
            "ADV : Epoch: 18 || Train Loss: 2.103692066363846,\t Train Acc: 0.2900440705128205 || Test Loss: 2.0079018030410256,\t Test Accuracy: 0.3016826923076923\n",
            "ADV : Epoch: 19 || Train Loss: 2.045923769474103,\t Train Acc: 0.29517227564102566 || Test Loss: 1.911975336380769,\t Test Accuracy: 0.3249198717948718\n",
            "BASELINE: Epoch: 0 || Train Loss: 109.49849664737026,\t Train Acc: 0.10606971153846154 || Test Loss: 1.9147675511166666,\t Test Accuracy: 0.32642227564102566\n",
            "BASELINE: Epoch: 1 || Train Loss: 22.783457088470517,\t Train Acc: 0.12471955128205128 || Test Loss: 1.9118475058141025,\t Test Accuracy: 0.3310296474358974\n",
            "BASELINE: Epoch: 2 || Train Loss: 11.988779169473846,\t Train Acc: 0.13892227564102563 || Test Loss: 1.9162066716410255,\t Test Accuracy: 0.3249198717948718\n",
            "BASELINE: Epoch: 3 || Train Loss: 7.135083608138205,\t Train Acc: 0.1681290064102564 || Test Loss: 1.9143487597128206,\t Test Accuracy: 0.32832532051282054\n",
            "BASELINE: Epoch: 4 || Train Loss: 5.23426986229718,\t Train Acc: 0.16177884615384616 || Test Loss: 1.9160184386448715,\t Test Accuracy: 0.3210136217948718\n",
            "BASELINE: Epoch: 5 || Train Loss: 4.01584436710077,\t Train Acc: 0.1625 || Test Loss: 1.9161776701615385,\t Test Accuracy: 0.32532051282051283\n",
            "BASELINE: Epoch: 6 || Train Loss: 3.3791579686679487,\t Train Acc: 0.17772435897435898 || Test Loss: 1.9161187746589745,\t Test Accuracy: 0.32411858974358976\n",
            "BASELINE: Epoch: 7 || Train Loss: 2.997112257052564,\t Train Acc: 0.18984375 || Test Loss: 1.9166488158397437,\t Test Accuracy: 0.3258213141025641\n",
            "BASELINE: Epoch: 8 || Train Loss: 2.82856313754359,\t Train Acc: 0.20032051282051283 || Test Loss: 1.91561072300641,\t Test Accuracy: 0.3267227564102564\n",
            "BASELINE: Epoch: 9 || Train Loss: 2.6838238098682052,\t Train Acc: 0.21985176282051283 || Test Loss: 1.9113205839423075,\t Test Accuracy: 0.32471955128205127\n",
            "BASELINE: Epoch: 10 || Train Loss: 2.499863205811795,\t Train Acc: 0.23147035256410256 || Test Loss: 1.9225180989653847,\t Test Accuracy: 0.32892628205128205\n",
            "BASELINE: Epoch: 11 || Train Loss: 2.3919984456820513,\t Train Acc: 0.2365584935897436 || Test Loss: 1.9124804139141025,\t Test Accuracy: 0.32592147435897434\n",
            "BASELINE: Epoch: 12 || Train Loss: 2.31532515104,\t Train Acc: 0.24491185897435896 || Test Loss: 1.9154341343115384,\t Test Accuracy: 0.3256209935897436\n",
            "BASELINE: Epoch: 13 || Train Loss: 2.4181674972558973,\t Train Acc: 0.24755608974358975 || Test Loss: 1.9125059048333333,\t Test Accuracy: 0.32982772435897434\n",
            "BASELINE: Epoch: 14 || Train Loss: 2.8312758962312823,\t Train Acc: 0.22922676282051282 || Test Loss: 1.9170017899615386,\t Test Accuracy: 0.32592147435897434\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "BASELINE: Epoch: 15 || Train Loss: 3.271272500356154,\t Train Acc: 0.21776842948717948 || Test Loss: 1.9079690881269231,\t Test Accuracy: 0.3332331730769231\n",
            "BASELINE: Epoch: 16 || Train Loss: 2.8638394239620517,\t Train Acc: 0.22884615384615384 || Test Loss: 1.9226119197333333,\t Test Accuracy: 0.3271233974358974\n",
            "BASELINE: Epoch: 17 || Train Loss: 2.4004999240238463,\t Train Acc: 0.2481971153846154 || Test Loss: 1.9106852977705127,\t Test Accuracy: 0.32962740384615385\n",
            "BASELINE: Epoch: 18 || Train Loss: 2.2587730325184614,\t Train Acc: 0.26213942307692306 || Test Loss: 1.9173699632666665,\t Test Accuracy: 0.32502003205128205\n",
            "BASELINE: Epoch: 19 || Train Loss: 2.241268974389744,\t Train Acc: 0.2563301282051282 || Test Loss: 1.9153738251102566,\t Test Accuracy: 0.32862580128205127\n",
            "Accuracy_BATCH_SIZE128EPOCHS20STEPS390TYPEtrain.png\n",
            "Loss_BATCH_SIZE128EPOCHS20STEPS390TYPEtrain.png\n",
            "Accuracy_BATCH_SIZE128EPOCHS20STEPS78TYPEtest.png\n",
            "Loss_BATCH_SIZE128EPOCHS20STEPS78TYPEtest.png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.1114383 , 0.12061298, 0.13565705, 0.15494792, 0.15392628,\n",
              "        0.15891426, 0.17866587, 0.19022436, 0.20484776, 0.21784856,\n",
              "        0.23317308, 0.24160657, 0.25340545, 0.2635617 , 0.24407051,\n",
              "        0.26137821, 0.27441907, 0.2698117 , 0.29004407, 0.29517228]),\n",
              " array([113.71768614,  11.53870524,   9.07605478,   6.55038469,\n",
              "          6.50112327,   4.73848875,   3.5721077 ,   2.92532021,\n",
              "          2.76799796,   2.5228058 ,   2.38865193,   2.38833162,\n",
              "          2.32570965,   2.2572906 ,   2.47679444,   2.29127227,\n",
              "          2.17742566,   2.14954419,   2.10369207,   2.04592377]),\n",
              " array([0.10847356, 0.15354567, 0.15324519, 0.14973958, 0.1494391 ,\n",
              "        0.16836939, 0.17958734, 0.18689904, 0.19310897, 0.21464343,\n",
              "        0.27834535, 0.26943109, 0.29136619, 0.23347356, 0.27013221,\n",
              "        0.24288862, 0.30859375, 0.27614183, 0.30168269, 0.32491987]),\n",
              " array([16.5146625 ,  9.35540312,  5.30055689,  6.44128741,  5.4553591 ,\n",
              "         3.0965797 ,  3.49150112,  2.71281323,  2.65621662,  2.37148798,\n",
              "         2.10697123,  2.07203203,  2.05008368,  2.29660971,  2.38355189,\n",
              "         2.19253338,  2.18484263,  2.11202042,  2.0079018 ,  1.91197534]),\n",
              " array([0.10606971, 0.12471955, 0.13892228, 0.16812901, 0.16177885,\n",
              "        0.1625    , 0.17772436, 0.18984375, 0.20032051, 0.21985176,\n",
              "        0.23147035, 0.23655849, 0.24491186, 0.24755609, 0.22922676,\n",
              "        0.21776843, 0.22884615, 0.24819712, 0.26213942, 0.25633013]),\n",
              " array([109.49849665,  22.78345709,  11.98877917,   7.13508361,\n",
              "          5.23426986,   4.01584437,   3.37915797,   2.99711226,\n",
              "          2.82856314,   2.68382381,   2.49986321,   2.39199845,\n",
              "          2.31532515,   2.4181675 ,   2.8312759 ,   3.2712725 ,\n",
              "          2.86383942,   2.40049992,   2.25877303,   2.24126897]),\n",
              " array([0.32642228, 0.33102965, 0.32491987, 0.32832532, 0.32101362,\n",
              "        0.32532051, 0.32411859, 0.32582131, 0.32672276, 0.32471955,\n",
              "        0.32892628, 0.32592147, 0.32562099, 0.32982772, 0.32592147,\n",
              "        0.33323317, 0.3271234 , 0.3296274 , 0.32502003, 0.3286258 ]),\n",
              " array([1.91476755, 1.91184751, 1.91620667, 1.91434876, 1.91601844,\n",
              "        1.91617767, 1.91611877, 1.91664882, 1.91561072, 1.91132058,\n",
              "        1.9225181 , 1.91248041, 1.91543413, 1.9125059 , 1.91700179,\n",
              "        1.90796909, 1.92261192, 1.9106853 , 1.91736996, 1.91537383]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "9WaFKlEmKupB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "733f18e7-04ee-4641-d025-3483293d02dc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523818763312,
          "user_tz": 240,
          "elapsed": 224,
          "user": {
            "displayName": "Tiffany Wang",
            "photoUrl": "//lh3.googleusercontent.com/-DxivnaZLcsI/AAAAAAAAAAI/AAAAAAAAH_I/QDov37LUsKk/s50-c-k-no/photo.jpg",
            "userId": "108532619344838745606"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "os.listdir('.')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['datalab',\n",
              " 'Accuracy_BATCH_SIZE28EPOCHS20STEPS3TYPEtrain.png',\n",
              " '.cache',\n",
              " '.forever',\n",
              " 'Loss_BATCH_SIZE128EPOCHS20STEPS390TYPEtrain.png',\n",
              " '.nv',\n",
              " 'Loss_BATCH_SIZE28EPOCHS20STEPS3TYPEtrain.png',\n",
              " 'Loss_BATCH_SIZE28EPOCHS20STEPS3TYPEtest.png',\n",
              " '.local',\n",
              " '.keras',\n",
              " '.config',\n",
              " 'Accuracy_BATCH_SIZE128EPOCHS20STEPS390TYPEtrain.png',\n",
              " 'Accuracy_BATCH_SIZE28EPOCHS20STEPS3TYPEtest.png',\n",
              " '.ipython',\n",
              " 'Loss_BATCH_SIZE128EPOCHS20STEPS78TYPEtest.png',\n",
              " '.rnd',\n",
              " 'Accuracy_BATCH_SIZE128EPOCHS20STEPS78TYPEtest.png']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "19EwxO-HI3Ax",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "files.download('Loss_BATCH_SIZE28EPOCHS20STEPS3TYPEtrain.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hZjIIPhEgsZH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "files.download('Accuracy_BATCH_SIZE128EPOCHS20STEPS78TYPEtest.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r2oUo8gpxeR7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "files.download('Accuracy_BATCH_SIZE28EPOCHS20STEPS3TYPEtest.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-HZtyDnfxkRq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "files.download('Loss_BATCH_SIZE256EPOCHS20STEPS39BASELINETrueTYPEtest.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cv9tZXB-ABH5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "files.download('lala.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CiYggdPNAEC6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "3a58f39c-3cc2-4b85-f738-4890a6aeb4b0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523589381410,
          "user_tz": 240,
          "elapsed": 1165,
          "user": {
            "displayName": "Tiffany Wang",
            "photoUrl": "//lh3.googleusercontent.com/-DxivnaZLcsI/AAAAAAAAAAI/AAAAAAAAH_I/QDov37LUsKk/s50-c-k-no/photo.jpg",
            "userId": "108532619344838745606"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "a = [1, 2, 3, 4, 5]\n",
        "b = [2, 3, 4, 5, 6]\n",
        "\n",
        "plt.plot(a, b)\n",
        "plt.savefig('okok.png')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmOT/B/UH11uTbt68AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f70b697f0f0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "HcxJUr7VAPhM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "24910b0a-8004-422f-8545-2b08999b1283",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523589386597,
          "user_tz": 240,
          "elapsed": 1763,
          "user": {
            "displayName": "Tiffany Wang",
            "photoUrl": "//lh3.googleusercontent.com/-DxivnaZLcsI/AAAAAAAAAAI/AAAAAAAAH_I/QDov37LUsKk/s50-c-k-no/photo.jpg",
            "userId": "108532619344838745606"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy_BATCH_SIZE128EPOCHS10STEPS390BASELINEFalseTYPEtrain.png\r\n",
            "Accuracy_BATCH_SIZE128EPOCHS1STEPS4BASELINEFalseTYPEtest.png\r\n",
            "Accuracy_BATCH_SIZE128EPOCHS1STEPS4BASELINEFalseTYPEtrain.png\r\n",
            "\u001b[0m\u001b[01;34mdatalab\u001b[0m/\r\n",
            "lala.png\r\n",
            "Loss_BATCH_SIZE128EPOCHS1STEPS4BASELINEFalseTYPEtest.png\r\n",
            "Loss_BATCH_SIZE128EPOCHS1STEPS4BASELINEFalseTYPEtrain.png\r\n",
            "okok.png\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hs3X6Cr5AQo8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "files.download('okok.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y1ToKVfKASkq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}